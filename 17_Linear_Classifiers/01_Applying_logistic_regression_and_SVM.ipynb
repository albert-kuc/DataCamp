{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Applying logistic regression and SVM:\n",
    "In this chapter you will learn the basics of applying logistic regression and support vector machines (SVMs) to classification problems. You'll use the scikit-learn library to fit classification models to real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. scikit-learn refresher\n",
    "## 1.1 KNN classification\n",
    "In this exercise you'll explore a subset of the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/). The variables `X_train`, `X_test`, `y_train`, and `y_test` are already loaded into the environment. The `X` variables contain features based on the words in the movie reviews, and the `y` variables contain labels for whether the review sentiment is positive (+1) or negative (-1).\n",
    "\n",
    "### Instructions:\n",
    "* Create a KNN model with default hyperparameters.\n",
    "* Fit the model.\n",
    "* Print out the prediction for the test example 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "DATADIR='_datasets/'\n",
    "X_train, y_train = load_svmlight_file(DATADIR+'reviews_train_smaller_pm1.txt')\n",
    "X_test,  y_test  = load_svmlight_file(DATADIR+'reviews_test_smaller_pm1.txt')\n",
    "\n",
    "# remove some examples, otherwise KNN is too slow\n",
    "n = 2000\n",
    "X_train = X_train[:n]\n",
    "y_train = y_train[:n]\n",
    "X_test = X_test[:n]\n",
    "y_test = y_test[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test example 0: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and fit the model\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "pred = knn.predict(X_test)[0]\n",
    "print(\"Prediction for test example 0:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Comparing models\n",
    "Compare k nearest neighbors classifiers with k=1 and k=5 on the handwritten digits data set, which is already loaded into the variables `X_train`, `y_train`, `X_test`, and `y_test`. You can set k with the `n_neighbors` parameter when creating the `KNeighborsClassifier` object, which is also already imported into the environment.\n",
    "\n",
    "Which model has a higher test accuracy?\n",
    "\n",
    "Possible Answers:\n",
    "1. k=1\n",
    "2. k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "digits = sklearn.datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9888888888888889"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(X_train, y_train)\n",
    "knn1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train, y_train)\n",
    "knn5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Overfitting\n",
    "Which of the following situations looks like an example of overfitting?\n",
    "\n",
    "Possible Answers\n",
    "1. Training accuracy 50%, testing accuracy 50%.\n",
    "2. Training accuracy 95%, testing accuracy 95%.\n",
    "3. Training accuracy 95%, testing accuracy 50%.\n",
    "4. Training accuracy 50%, testing accuracy 95%.\n",
    "\n",
    "<div style=\"text-align: right\"> Answer: 3 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applying logistic regression and SVM\n",
    "## 2.1 Running LogisticRegression and SVC\n",
    "In this exercise, you'll apply logistic regression and a support vector machine to classify images of handwritten digits.\n",
    "\n",
    "### Instructions:\n",
    "* Apply logistic regression and SVM (using `SVC()`) to the handwritten digits data set using the provided train/validation split.\n",
    "* For each classifier, print out the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.994060876020787\n",
      "0.9622222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression(solver='liblinear', multi_class='auto') # solver and multi_class parameter set to silence warning\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4111111111111111\n"
     ]
    }
   ],
   "source": [
    "# Apply SVM and print scores\n",
    "svm = SVC(gamma='auto') # gamma parameter set to silence warning\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in the course we'll look at the similarities and differences of logistic regression vs. SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sentiment analysis for movie reviews\n",
    "In this exercise you'll explore the probabilities outputted by logistic regression on a subset of the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "The variables `X` and `y` are already loaded into the environment. `X` contains features based on the number of times words appear in the movie reviews, and `y` contains labels for whether the review sentiment is positive (+1) or negative (-1).\n",
    "\n",
    "### Instructions:\n",
    "* Train a logistic regression model on the movie review data.\n",
    "* Predict the probabilities of negative vs. positive for the two given reviews.\n",
    "* Feel free to write your own reviews and get probabilities for those too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "DATADIR='_datasets/'\n",
    "X, y = load_svmlight_file(DATADIR+'reviews_train_smaller_pm1.txt')\n",
    "#X_test,  y_test  = load_svmlight_file(DATADIR+'reviews_test_smaller_pm1.txt')\n",
    "\n",
    "# Data is too big\n",
    "X = X[:2000,]\n",
    "y = y[:2000,]\n",
    "\n",
    "with open('_datasets/reviews_vocab.txt') as f:\n",
    "    vocab = f.read().splitlines()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "\n",
    "def get_features(review):\n",
    "    return vectorizer.transform([review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
      "Probability of positive review: 0.8079272805911303\n",
      "Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n",
      "Probability of positive review: 0.5855352242119005\n"
     ]
    }
   ],
   "source": [
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression(solver='liblinear') # solver parameter set to silence warning\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "review2_features = get_features(review2)\n",
    "print(\"Review:\", review2)\n",
    "print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second probability would have been even lower, but the word \"good\" trips it up a bit, since that's considered a \"positive\" word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear classifiers\n",
    "## 3.1 Which decision boundary is linear?\n",
    "Which of the following is a linear decision boundary?\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/course_6199/datasets/multiple_choce_linear_boundary.png\" alt=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Answer: 1</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Visualizing decision boundaries\n",
    "In this exercise, you'll visualize the decision boundaries of various classifier types.\n",
    "\n",
    "A subset of `scikit-learn`'s built-in `wine` dataset is already loaded into `X`, along with binary labels in `y`.\n",
    "\n",
    "### Instructions:\n",
    "* Create the following classifier objects with default hyperparameters: `LogisticRegression`, `LinearSVC`, `SVC`, `KNeighborsClassifier`.\n",
    "* Fit each of the classifiers on the provided data using a `for` loop.\n",
    "* Call the `plot_4_classifers()` function (similar to the code [here](https://scikit-learn.org/stable/auto_examples/svm/plot_iris.html)), passing in `X`, `y`, and a list containing the four classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python37\\Mikele\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# from plot_classifier import *\n",
    "\"\"\"Help on function plot_classifier in module plot_classifier:\n",
    "\n",
    "plot_classifier(X, y, clf, ax=None, ticks=False, proba=False, lims=None)\n",
    "    # adapted from http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\"\"\"\n",
    "    \n",
    "\n",
    "data = datasets.load_wine()\n",
    "X = data.data[:, :2] # Take the first two features, for visualization purposes\n",
    "y = data.target\n",
    "y = y > 0 # turn into binary classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, _, y, _ = train_test_split(X, y, train_size=50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python37\\Mikele\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Python37\\Mikele\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Python37\\Mikele\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHelp on function plot_4_classifiers in module plot_classifier:\\n\\nplot_4_classifiers(X, y, clfs)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers\n",
    "# plot_4_classifiers(X, y, classifiers)\n",
    "\"\"\"\n",
    "Help on function plot_4_classifiers in module plot_classifier:\n",
    "\n",
    "plot_4_classifiers(X, y, clfs)\n",
    "\"\"\"\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
